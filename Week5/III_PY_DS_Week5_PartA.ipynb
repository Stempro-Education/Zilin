{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 bold=true>Jan 5, 2020 -- Part A\n",
    "<br><br>Content Acquisition</font>\n",
    "\n",
    "<font size=5 color='Olive'>For the first 5 lessons, DS will be combined with Python due to insufficiencies of Concepts</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T07:12:58.186170Z",
     "start_time": "2020-01-05T07:12:58.139172Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, Latex \n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import requests\n",
    "import ntpath\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "from pprint import pprint \n",
    "import shutil\n",
    "#!pip install tldextract\n",
    "import tldextract\n",
    "\n",
    "'''\n",
    "    c=CollegeCrawl('University of Washington', 'https://www.washington.edu')\n",
    "    c.GetAllUrls()\n",
    "'''\n",
    "class CollegeCrawl():\n",
    "    Gap_Insecond=1\n",
    "    Max_Pages=15      \n",
    "    \"\"\"\n",
    "        collegename: name\n",
    "        rooturl: https://www.university.edu. Scheme and netloc need to be complete\n",
    "        prioritykeywords: ['apply','adimission'...] etc. if None then everth page \n",
    "        respectrobottxt: True\n",
    "        use tldextract instead of urlparse to get the domain name. --changed on Jan 4 2020\n",
    "    \"\"\"\n",
    "    def __init__(self,_collegename, _rooturl, \n",
    "                 _prioritykeywords=['apply', 'admission', 'application', 'deadline'], \n",
    "                 _url_file=None,                  \n",
    "                 _save_to_folder=None,\n",
    "                 _existingurlfile=None, #csv files that were visited in the past\n",
    "                 _respectrobottxt=True, \n",
    "                _headers={'User-Agent':'Mozilla/5.0'} ):\n",
    "        self.college=_collegename\n",
    "         \n",
    "        if urlparse(_rooturl).scheme==\"\":\n",
    "            print('URL needs to have scheme. Please try again.')\n",
    "            raise Exception('URL needs to have a scheme')          \n",
    "        \n",
    "        self.rootUrl=_rooturl\n",
    "        #self.base_domain=urlparse(self.rootUrl).netloc\n",
    "        self.base_domain=tldextract.extract(self.rootUrl).domain\n",
    "        self.scheme=urlparse(self.rootUrl).scheme\n",
    "        self.priorityKeywords=_prioritykeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "        if _save_to_folder==None or path.isdir(_save_to_folder)==False:\n",
    "            self.save_to_folder=os.getcwd()\n",
    "        else:\n",
    "            self.save_to_folder=_save_to_folder\n",
    "            \n",
    "        # to make it less \n",
    "        if _existingurlfile==None or path.exists(_existingurlfile)==False:      \n",
    "            self.existingurlfile=path.join(self.save_to_folder,re.sub(r\"\\s+\", \"_\", self.college.strip()+'.csv'))\n",
    "        else:\n",
    "            self.existingurlfile=_existingurlfile\n",
    "        self.allurls={}\n",
    "        self.headers=_headers\n",
    "        self.files=[]\n",
    "            \n",
    "    '''simple description'''        \n",
    "    def __str__(self):\n",
    "        return '{}. Starting URL: {}'.format(self.college, self.rootUrl)\n",
    "    '''\n",
    "        load _existingurlfile: two columns -- Url and status_code\n",
    "        minimum assumptions: first two columns are url and status_code\n",
    "    '''\n",
    "    def Load_DiscoveredUrls(self, delimiter='\\t', hasHeader=False, header_names=['url', 'status_code']):   \n",
    "        if self.existingurlfile==None:\n",
    "            return dict()\n",
    "        else:\n",
    "            if path.exists(self.existingurlfile) and re.sub(r'\\s+', '_', self.college) \\\n",
    "                in ntpath.basename(self.existingurlfile):\n",
    "                df_urls=pd.read_csv(self.existingurlfile,  delimiter=delimiter, names=['url', 'status_code'])\n",
    "                df_urls=df_urls[df_urls['url']!='url']  \n",
    "                return dict(zip(df_urls['url'], df_urls['status_code'])) #format: url:status_code. i.e., url is the key\n",
    "            else:\n",
    "                return dict() \n",
    "            \n",
    "    \"\"\"\n",
    "        get all urls starting from rootUrl \n",
    "        headers={'User-Agent':'Mozilla/5.0'}\n",
    "        #Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36\n",
    "        #full list: https://www.whatismybrowser.com/guides/the-latest-user-agent/chrome\n",
    "        response= requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text)  \n",
    "        \n",
    "        it will first load which ever has alread visited \n",
    "\n",
    "    \"\"\"    \n",
    "    def GetAllUrls(self, headers=None, links_only=True): \n",
    "        #load existing urls if any\n",
    "        urls=self.Load_DiscoveredUrls() \n",
    "        \n",
    "        if len(urls)==0:\n",
    "            urls={self.rootUrl:'0'}  \n",
    "            unvisited=[self.rootUrl]\n",
    "        else:\n",
    "            unvisited=[url for url, status_code in urls.items() if str(status_code)=='0'] \n",
    "            \n",
    "            if len(unvisited)==0:\n",
    "                unvisited=[self.rootUrl] \n",
    "            else:\n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*10+10)/len(item))\n",
    "                \n",
    "        if headers==None: \n",
    "            if self.headers:\n",
    "                headers=self.headers\n",
    "            else: \n",
    "                 headers={'User-Agent':'Mozilla/5.0'}  \n",
    "       \n",
    "        #get base_domain    \n",
    "        if self.base_domain==None:\n",
    "            self.base_domain=urlparse(unvisited[0]).netloc\n",
    "        if self.scheme==None:\n",
    "            self.scheme=urlparse(unvisited[0]).scheme\n",
    "       \n",
    "        pages_visited =0\n",
    "         \n",
    "        try:  \n",
    "            while len(unvisited)>0:        \n",
    "                pages_visited+=1\n",
    "                url=unvisited.pop()    \n",
    "                url_parsed=urlparse(url)\n",
    "                url_domain_path='{uri.scheme}://{uri.netloc}'.format(uri=url_parsed)\n",
    "              \n",
    "                response=requests.get(url, headers=headers)     \n",
    "                status_code=response.status_code\n",
    "                \n",
    "                urls[url]=status_code \n",
    "                 \n",
    "                if status_code==200: \n",
    "                    soup=BeautifulSoup(response.text,  'lxml') #'html.parser')  \n",
    "                    for link in soup.find_all('a'):  \n",
    "                        if link.has_attr('href'):\n",
    "                            link_url=link['href']    \n",
    "                            \n",
    "                            if not re.match('.+@.+', link_url ):\n",
    "                                link_url=re.sub(r'[\\/_+!@#$?\\\\\\s]+$', '', link_url)\n",
    "\n",
    "                                parsed_uri_path=urlparse(link_url) \n",
    "                                extract_uri_domain=tldextract.extract(link_url)                            \n",
    "                                #parts: parsed_uri.domain + '.' + parsed_uri.suffix  \n",
    "                                absolute_url=''                                 \n",
    "                                if  (extract_uri_domain.domain=='') and re.match(r'^\\/.*\\w$', parsed_uri_path.path) :\n",
    "                                    absolute_url=urljoin(url_domain_path,parsed_uri_path.path)\n",
    "                                elif extract_uri_domain.domain==self.base_domain: # and re.match('^http', parsed_uri.scheme):\n",
    "                                    if re.match(r'^\\/\\/', link_url):\n",
    "                                        absolute_url=self.scheme+':'+link_url\n",
    "                                    else:\n",
    "                                        absolute_url=link_url\n",
    "                                else:\n",
    "                                    continue    \n",
    "\n",
    "                                if absolute_url!='' and absolute_url not in urls:   \n",
    "                                     urls[absolute_url]='0' \n",
    "                    self.SaveToCsv_FromResponse(url, response)  \n",
    "              \n",
    "                if pages_visited>=self.Max_Pages:\n",
    "                    break   \n",
    "                \n",
    "                unvisited=[name for name, code in urls.items() if str(code)=='0']    \n",
    "                #sorting rule: has keywords, short url, else\n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*10+10)/len(item))\n",
    "                time.sleep(self.Gap_Insecond) #wait for few seconds.  \n",
    "   \n",
    "        except: \n",
    "            print('url \"{}\" went wrong'.format(url))  \n",
    "            urls[url]='999'\n",
    "                #not to consider failed pages. status_code 400s may need manual handling of they are high priority pages\n",
    "        finally: \n",
    "            self.allurls=urls\n",
    "            #csv_columns = ['url', 'status_code']  \n",
    "            #try:\n",
    "                #with open(self.existingurlfile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    #writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "                    #writer.writeheader()\n",
    "                    #for data in urls:\n",
    "                        #writer.writerow(data)\n",
    "            #except IOError:\n",
    "                #print(\"I/O error\")\n",
    "    \n",
    "            self.Save_Summaries() \n",
    "            \n",
    "    \"\"\"\n",
    "        display summaries \n",
    "    \"\"\"\n",
    "    \n",
    "    def Save_Summaries(self): \n",
    "        if self.allurls:\n",
    "            df_urls=pd.DataFrame([[i, k] for i, k in (zip(self.allurls.keys(), self.allurls.values()) )], columns=['url', 'status_code'])\n",
    "            \n",
    "            #df_urls=pd.DataFrame(list(c.allurls.items()), columns=['url', 'status_code'])\n",
    "            print('Summary for college ', self.college)\n",
    "            print('\\n')\n",
    "            print(df_urls.groupby('status_code').count().reset_index())\n",
    "            #save file as well  \n",
    "            try:\n",
    "                with open(self.existingurlfile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                    writer.writerow(['url','status_code'])\n",
    "                    for url, status_code in self.allurls.items():  \n",
    "                        writer.writerow([url, status_code])  \n",
    "            except IOError:\n",
    "                print('IO Error in saving summaries')\n",
    "                \n",
    "        if self.files:\n",
    "            print('\\nThe following {} file(s) are generated. '.format(len(self.files)))\n",
    "            pprint(self.files) \n",
    "    \"\"\"\n",
    "        read one page\n",
    "    \"\"\"\n",
    "    def Read_Oneurl(self, url):  \n",
    "        response=requests.get(url, self.headers)  \n",
    "        if response.status_code==200: \n",
    "            return self.Get_Pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "    \n",
    "    '''\n",
    "        used for filter()\n",
    "    '''\n",
    "    def Tag_Visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    "\n",
    "    '''\n",
    "        get all text from page body\n",
    "    '''\n",
    "    def Get_Pagetext(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #conent\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.Tag_Visible, texts)       \n",
    "\n",
    "        return [ [t.parent.name,   \n",
    "                 t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "                 t.nextSibling.name if t.nextSibling!=None else None,\n",
    "                 re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2]\n",
    "\n",
    "    '''\n",
    "        save One Page\n",
    "        filename, if not None, should not be full name. use import ntpath ntpath.basename(\"a/b/c\")\n",
    "    '''\n",
    "    def SaveToCsv_FromUrl(self,url): # tab delimiter only  \n",
    "        content=self.Read_Oneurl(url.strip()) #in format of (a,a,a,a)   \n",
    "        self.SaveToCsv(url, content)\n",
    "        #return SaveToCsv_FromResponse()\n",
    "    \n",
    "    '''\n",
    "        save from resonse. called in the initial loop\n",
    "    '''\n",
    "    def SaveToCsv_FromResponse(self, url, response): \n",
    "        content= self.Get_Pagetext(response)\n",
    "        self.SaveToCsv(url, content) \n",
    "        \n",
    "    '''\n",
    "        save to csv file and append it to self.files\n",
    "    '''    \n",
    "    def SaveToCsv(self, url, content):\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "        fullname=path.join(self.save_to_folder, filename)\n",
    "        try:\n",
    "            with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "                for lll in content: \n",
    "                    lll.insert(0, url)\n",
    "                    writer.writerow(lll)  \n",
    "            self.files.append(fullname) \n",
    "        except IOError:\n",
    "            print('failed to save file.')\n",
    "            \n",
    "    '''\n",
    "        move file from one folder to another \n",
    "        pattern: regular expression\n",
    "    '''\n",
    "    def MoveFiles(self, destination_folder, source_folder=None,  pattern=None): \n",
    "        if source_folder==None:\n",
    "            source_folder=self.save_to_folder\n",
    "        if source_folder==None:\n",
    "            print('Please specify folder with the files. ')\n",
    "            return\n",
    "\n",
    "        if destination_folder==None or path.isdir(destination_folder)==False:\n",
    "            print('Need a valid destination folder')\n",
    "            return\n",
    "        if pattern==None or pattern==\"\":\n",
    "            pattern=\"csv$\" \n",
    "        files = [f for f in os.listdir(self.save_to_folder) if re.match(pattern, f) and path.isfile(path.join(source_folder, f))]\n",
    "        for f in files:\n",
    "            shutil.move(path.join(source_folder, f), destination_folder)  \n",
    "    \n",
    "    '''\n",
    "        Merger files into one consolidated file by college. \n",
    "        drop duplicates \n",
    "        use relative folder path only. ./file name\n",
    "    '''\n",
    "    def Merge(self, merged_file_folder=None, isRemoveRawFile=False ): \n",
    "        if merged_file_folder==None:\n",
    "            merged_file_folder='./training'  \n",
    "        \n",
    "        elif not re.match(r'^\\.',merged_file_folder):\n",
    "            print('Please provide a destination folder for merged file. e.g., ./training')\n",
    "            return\n",
    "        #safely create file\n",
    "        Path(path.join(os.getcwd(), merged_file_folder)).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        merged_file=path.join(path.split(self.existingurlfile)[0], 'merged_'+path.split(self.existingurlfile)[1])\n",
    "            \n",
    "        #loaded files\n",
    "        files=[(path.join(self.save_to_folder, f), 1)  for f in os.listdir(self.save_to_folder) if re.match(r'^http.+\\d+.+csv$', f)]\n",
    "\n",
    "        if path.exists(merged_file):\n",
    "            files.append((merged_file, 0))\n",
    "\n",
    "        df_combined = pd.concat([pd.read_csv(f[0] , sep='\\t') for f in files])\n",
    "        #drop dups\n",
    "        df_combined.drop_duplicates(inplace=True)\n",
    "        df_combined.to_csv( merged_file, index=False, sep='\\t', encoding='utf-8')\n",
    "\n",
    "        if isRemoveRawFile:\n",
    "            for f in files:\n",
    "                if f[1]==1:\n",
    "                    os.remove(f[0])\n",
    "'''\n",
    "c=CollegeCrawl('stanford', 'https://www.stanford.edu')\n",
    "ab=c.GetAllUrls()  #https://admission.stanford.edu/apply/\n",
    "c.Merge(isRemoveRawFile=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T07:16:41.820707Z",
     "start_time": "2020-01-05T07:16:21.414009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for college  stanford\n",
      "\n",
      "\n",
      "  status_code  url\n",
      "0         200   15\n",
      "1           0  759\n",
      "2         200   48\n",
      "\n",
      "The following 15 file(s) are generated. \n",
      "['C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___gradadmissions_dot_stanford_dot_edu_applying_starting-your-application_your-account_01_04_2020_23_16_21.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___gradadmissions_dot_stanford_dot_edu_applying_starting-your-application_required-exams_01_04_2020_23_16_22.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___gradadmissions_dot_stanford_dot_edu_degrees_01_04_2020_23_16_24.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___gradadmissions_dot_stanford_dot_edu_applying_starting-your-application_required-exams_01_04_2020_23_16_25.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___gradadmissions_dot_stanford_dot_edu_degrees_01_04_2020_23_16_26.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___gradadmissions_dot_stanford_dot_edu_programs_01_04_2020_23_16_28.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___admission_dot_stanford_dot_edu_publications_01_04_2020_23_16_29.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___gradadmissions_dot_stanford_dot_edu_applying_decision-notification_01_04_2020_23_16_30.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___gradadmissions_dot_stanford_dot_edu_applying_starting-your-application_policy-statements_01_04_2020_23_16_31.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___gradadmissions_dot_stanford_dot_edu_applying_decision-notification_01_04_2020_23_16_33.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___gradadmissions_dot_stanford_dot_edu_programs_01_04_2020_23_16_34.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___gradadmissions_dot_stanford_dot_edu_applying_starting-your-application_policy-statements_01_04_2020_23_16_35.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___gradadmissions_dot_stanford_dot_edu_sso_login_01_04_2020_23_16_37.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___ed_dot_stanford_dot_edu_01_04_2020_23_16_39.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___ed_dot_stanford_dot_edu_admissions_apply_01_04_2020_23_16_41.csv']\n"
     ]
    }
   ],
   "source": [
    "c=CollegeCrawl('stanford', 'https://www.stanford.edu')\n",
    "c.GetAllUrls()  #https://admission.stanford.edu/apply/\n",
    "c.Merge(isRemoveRawFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
