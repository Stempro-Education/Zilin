{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 bold=true>Jan 5, 2020 -- Part A\n",
    "<br><br>Content Acquisition</font>\n",
    "\n",
    "<font size=5 color='Olive'>For the first 5 lessons, DS will be combined with Python due to insufficiencies of Concepts</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T06:44:44.674353Z",
     "start_time": "2020-01-09T06:44:44.625239Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, Latex \n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import requests\n",
    "import ntpath\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "from pprint import pprint \n",
    "import shutil\n",
    "#!pip install tldextract\n",
    "import tldextract\n",
    "\n",
    "'''\n",
    "    c=CollegeCrawl('University of Washington', 'https://www.washington.edu')\n",
    "    c.GetallUrls()\n",
    "'''\n",
    "class CollegeCrawl():  \n",
    "    \"\"\" Mandatory Input: \n",
    "            _collegeName: e.g., University of Toronto. \n",
    "                Targeted use case: input from batch file. \n",
    "            _rootUrl: e.g., https://www.uw.edu. \n",
    "                Targeted use case: input from batch file. \n",
    "                Both scheme and netloc has to be complete\n",
    "            _mergedFile: merge all crawled file, one csv per url, into one consolidated csv file.\n",
    "                Targeted use case: input from batch file.\n",
    "            _existingUrlFile: a csv file that stores url list. \n",
    "                Two columns: url and status code. \n",
    "                If the file doesnot exist, will generate it.\n",
    "                Targeted use case: input from batch file. \n",
    "        Optional Input:\n",
    "            _gapInSecond: if second >0 then the number of seconds. if -1 then 30 seconds\n",
    "            _maxPages: if -1 then 999 pages. if >0 then pages else 15 pages\n",
    "            _capUrls:  if -1 then 10000 pages. if >0 then  _capUrls else 2000 pages. Not to grab too many\n",
    "            _priorityKeywords=['apply', 'admission', 'application', 'deadline']\n",
    "                These words will be used for priority ranking. \n",
    "            _saveToFolder=None. Where to save the crawled csv files. \n",
    "                If None, then will take current work directory.  \n",
    "            _respectRobottxt=True, Not utilized for now. \n",
    "            _headers={'User-Agent':'Mozilla/5.0'}\n",
    "    \"\"\"        \n",
    "     \n",
    "    \n",
    "    def __init__(self,\n",
    "                 _collegeName, \n",
    "                 _rootUrl, \n",
    "                 _existingUrlFile, \n",
    "                 _mergedFile, \n",
    "                 _gapInSecond=1,\n",
    "                 _maxPages=15,\n",
    "                 _capUrls=2000,\n",
    "                 _priorityKeywords=['apply', 'admission', 'application', 'deadline'],  \n",
    "                 _saveToFolder=None, \n",
    "                 _respectrobottxt=True, \n",
    "                _headers={'User-Agent':'Mozilla/5.0'} ):\n",
    "        \"\"\" __init__: \n",
    "        Mandatory Input: \n",
    "            _collegeName: e.g., University of Toronto. \n",
    "                Targeted use case: input from batch file. \n",
    "            _rootUrl: e.g., https://www.uw.edu. \n",
    "                Targeted use case: input from batch file. \n",
    "                Both scheme and netloc has to be complete\n",
    "            _mergedFile: merge all crawled file, one csv per url, into one consolidated csv file.\n",
    "                Targeted use case: input from batch file.\n",
    "            _existingUrlFile: a csv file that stores url list. \n",
    "                Two columns: url and status code. \n",
    "                If the file doesnot exist, will generate it.\n",
    "                Targeted use case: input from batch file. \n",
    "        Optional Input:\n",
    "            _gapInSecond: if second >0 then the number of seconds. if -1 then 30 seconds\n",
    "            _maxPages: if -1 then 999 pages. if >0 then pages else 15 pages\n",
    "            _capUrls:  if -1 then 10000 pages. if >0 then  _capUrls else 2000 pages. Not to grab too many\n",
    "            _priorityKeywords=['apply', 'admission', 'application', 'deadline']\n",
    "                These words will be used for priority ranking. \n",
    "            _saveToFolder=None. Where to save the crawled csv files. \n",
    "                If None, then will take current work directory.  \n",
    "            _respectRobottxt=True, Not utilized for now. \n",
    "            _headers={'User-Agent':'Mozilla/5.0'}\n",
    "        \"\"\"\n",
    "            \n",
    "        self.collegeName=_collegeName\n",
    "         \n",
    "        if urlparse(_rootUrl).scheme==\"\":\n",
    "            print('URL needs to have scheme. Please try again.')\n",
    "            raise Exception('URL needs to have a scheme')          \n",
    "        \n",
    "        self.rootUrl=_rootUrl \n",
    "        self.existingUrlFile=_existingUrlFile\n",
    "        self.base_Domain=tldextract.extract(self.rootUrl).domain\n",
    "        self.scheme=urlparse(self.rootUrl).scheme\n",
    "        self.priorityKeywords=_priorityKeywords\n",
    "        self.respectRobottext=_respectrobottxt\n",
    "        \n",
    "        if _saveToFolder==None or path.isdir(_saveToFolder)==False:\n",
    "            self.saveToFolder=os.getcwd()\n",
    "        else:\n",
    "            self.saveToFolder=_saveToFolder\n",
    "        self.mergedFile=_mergedFile     \n",
    "        self.gapInSecond=_gapInSecond if _gapInSecond>0 else 30 if _gapInSecond==-1 else 0 \n",
    "        self.maxPages=_maxPages if _maxPages>0 else 999 if _maxPages==-1 else 15    \n",
    "        self.capUrls=_capUrls if _capUrls>0 else 10000 if _maxPages==-1 else 2000    \n",
    "        self.allUrls={}\n",
    "        self.headers=_headers\n",
    "        self.files=[]\n",
    "               \n",
    "    def __str__(self):\n",
    "        return '{}. Starting URL: {}'.format(self.collegeName, self.rootUrl)\n",
    "    '''\n",
    "        load _existingUrlFile: two columns -- Url and status_code\n",
    "        minimum assumptions: first two columns are url and status_code\n",
    "    '''\n",
    "    \n",
    "    def Load_DiscoveredUrls(self, delimiter='\\t', hasHeader=False, header_names=['url', 'status_code']):   \n",
    "        \"\"\"Load urls list (two columns: url(str) and status_code(str)) from self.existingUrlFile. \n",
    "           If the file existingUrlFile exists, then read. if not, load dict()\n",
    "           Parameters: \n",
    "               delimiter: by default is tab\n",
    "               hasHeader: the csv file has header\n",
    "               header_names: if any, url, status_code \n",
    "        \"\"\"\n",
    "        fullUrlFile=path.join(self.saveToFolder, self.existingUrlFile)\n",
    "        \n",
    "        if not path.exists(fullUrlFile): \n",
    "            return dict()\n",
    "        else: \n",
    "                df_urls=pd.read_csv(fullUrlFile,  delimiter=delimiter, names=['url', 'status_code'])\n",
    "                df_urls=df_urls[df_urls['url']!='url']  \n",
    "                return dict(zip(df_urls['url'], df_urls['status_code']))   \n",
    "                       \n",
    "             \n",
    "    def GetallUrls(self, headers=None, links_only=True): \n",
    "        \"\"\"\n",
    "           Get all urls from a college. \n",
    "           It carries the following steps: \n",
    "               1. Load_DiscoveredUrls\n",
    "               2. SaveToCsv_FromResponse\n",
    "               3. Show_Summaries(save=True) \n",
    "           visited url by priority of key words. \n",
    "           \n",
    "           Parameters: \n",
    "               headers: headers from web request. \n",
    "               links_only: only handle links.            \n",
    "        \"\"\" \n",
    "        urls=self.Load_DiscoveredUrls() \n",
    "        \n",
    "        if len(urls)==0:\n",
    "            urls={self.rootUrl:'0'}  \n",
    "            unvisited=[self.rootUrl]\n",
    "        else:\n",
    "            unvisited=[url for url, status_code in urls.items() if str(status_code)=='0'] \n",
    "            \n",
    "            if len(unvisited)==0:\n",
    "                unvisited=[self.rootUrl] \n",
    "            else:\n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*20+10)/len(item))\n",
    "                \n",
    "        if headers==None: \n",
    "            if self.headers:\n",
    "                headers=self.headers\n",
    "            else: \n",
    "                 headers={'User-Agent':'Mozilla/5.0'}  \n",
    "        \n",
    "        if self.base_Domain==None:\n",
    "            self.base_Domain=urlparse(unvisited[0]).netloc\n",
    "        if self.scheme==None:\n",
    "            self.scheme=urlparse(unvisited[0]).scheme\n",
    "       \n",
    "        pages_visited =0\n",
    "        \n",
    "        total0and200=len([url for url, status_code in urls.items() if str(status_code)=='0' or str(status_code)=='200'])\n",
    "        print('{}\\nTotal unvisited and successfully visited pages is {}. The max is approx. {}'.format(self.collegeName, total0and200, self.capUrls))\n",
    "        try:  \n",
    "            while len(unvisited)>0:        \n",
    "                pages_visited+=1\n",
    "                url=unvisited.pop()    \n",
    "                url_parsed=urlparse(url)\n",
    "                url_domain_path='{uri.scheme}://{uri.netloc}'.format(uri=url_parsed)\n",
    "              \n",
    "                response=requests.get(url, headers=headers)     \n",
    "                status_code=response.status_code\n",
    "                \n",
    "                urls[url]=str(status_code)\n",
    "                 \n",
    "                if status_code==200: \n",
    "                    soup=BeautifulSoup(response.text,  'lxml') #'html.parser')  \n",
    "                    \n",
    "                    if total0and200<self.capUrls:                        \n",
    "                        for link in soup.find_all('a'):  \n",
    "                            if link.has_attr('href'):\n",
    "                                link_url=link['href']    \n",
    "\n",
    "                                if not re.match('.+@.+', link_url ):\n",
    "                                    link_url=re.sub(r'[\\/_+!@#$?\\\\\\s]+$', '', link_url)\n",
    "\n",
    "                                    parsed_uri_path=urlparse(link_url) \n",
    "                                    extract_uri_domain=tldextract.extract(link_url)                            \n",
    "                                    #parts: parsed_uri.domain + '.' + parsed_uri.suffix  \n",
    "                                    absolute_url=''                                 \n",
    "                                    if  (extract_uri_domain.domain=='') and re.match(r'^\\/.*\\w$', parsed_uri_path.path) :\n",
    "                                        absolute_url=urljoin(url_domain_path,parsed_uri_path.path)\n",
    "                                    elif extract_uri_domain.domain==self.base_Domain: # and re.match('^http', parsed_uri.scheme):\n",
    "                                        if re.match(r'^\\/\\/', link_url):\n",
    "                                            absolute_url=self.scheme+':'+link_url\n",
    "                                        else:\n",
    "                                            absolute_url=link_url\n",
    "                                    else:\n",
    "                                        continue    \n",
    "\n",
    "                                    if absolute_url!='' and absolute_url not in urls:   \n",
    "                                         urls[absolute_url]='0' \n",
    "                    self.SaveToCsv_FromResponse(url, response)  \n",
    "                \n",
    "                if pages_visited>=self.maxPages:\n",
    "                    break   \n",
    "                \n",
    "                unvisited=[name for name, code in urls.items() if str(code)=='0']    \n",
    "                \n",
    "                unvisited=sorted(unvisited, key=lambda item: (sum([w in item for w in self.priorityKeywords])*10+10)/len(item))\n",
    "                if self.gapInSecond>0:\n",
    "                    time.sleep(self.gapInSecond)  \n",
    "   \n",
    "        except: # Exception as e: \n",
    "            #print('url \"{}\" went wrong. Error {} -- {}'.format(url,e.errno, e.strerror))   \n",
    "            urls[url]='999' \n",
    "            pass\n",
    "        finally: \n",
    "            self.allUrls=urls  \n",
    "         \n",
    "    def Show_Summaries(self, show=True): \n",
    "        \"\"\"\n",
    "            It will take one parameter: show\n",
    "            if show: \n",
    "                print out action summary\n",
    "            if not show: \n",
    "                return summary dataframe. this dataframe is digested by batch file to get stats. \n",
    "        \"\"\"\n",
    "        if self.allUrls:\n",
    "            df_urls=pd.DataFrame([[i, k] for i, k in (zip(self.allUrls.keys(), self.allUrls.values()) )], \\\n",
    "                                 columns=['url', 'status_code'])\n",
    "            \n",
    "            #df_urls=pd.DataFrame(list(c.allUrls.items()), columns=['url', 'status_code'])\n",
    "            \n",
    "             \n",
    "            try:\n",
    "                with open(self.existingUrlFile, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                    writer.writerow(['url','status_code'])\n",
    "                    for url, status_code in self.allUrls.items():  \n",
    "                        writer.writerow([url, status_code])  \n",
    "            except IOError as e: \n",
    "                print ((\"I/O Error({0}): {1} in saving summaries\").format(e.errno, e.strerror))\n",
    "                pass\n",
    "            finally:\n",
    "                df_sum=df_urls.groupby('status_code').count().reset_index()\n",
    "                if show==True:\n",
    "                    print('\\nSummary for college ', self.collegeName)\n",
    "                    print('\\nUrls Identified from college {}.'.format(self.collegeName))\n",
    "                    print(df_sum)\n",
    "                    print('\\n')\n",
    "                    print('\\nThe following {} file(s) are generated. '.format(len(self.files)))\n",
    "                    pprint(self.files) \n",
    "                return df_sum\n",
    "                  \n",
    "    def Read_Oneurl(self, url):  \n",
    "        \"\"\"\n",
    "            Get a response from request get call. \n",
    "            Followed by Get_Pagetext. \n",
    "        \"\"\"\n",
    "        response=requests.get(url, self.headers)  \n",
    "        if response.status_code==200: \n",
    "            return self.Get_Pagetext(response)\n",
    "        else: \n",
    "            return [[None, None, None, None]]\n",
    "     \n",
    "    def Tag_Visible(self, element):\n",
    "        \"\"\"\n",
    "            Filter tags: 'style', 'script', 'head', 'title', 'meta', '[document]'\n",
    "            This function is utilized by Get_Pagetext. \n",
    "        \"\"\"\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True \n",
    " \n",
    "    def Get_Pagetext(self, response):\n",
    "        \"\"\"\n",
    "            Get Text from the response text \n",
    "            Parameter: response. \n",
    "            Return a list of list: \n",
    "                [tagName,previousSibling, nextSibling, Text]\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(response.text, 'html.parser') #conent\n",
    "        texts = soup.findAll(text=True) \n",
    "        visible_texts = filter(self.Tag_Visible, texts)       \n",
    "\n",
    "        return [ [t.parent.name,   \n",
    "                 t.parent.previousSibling.name if t.parent.previousSibling!=None else None, \n",
    "                 t.nextSibling.name if t.nextSibling!=None else None,\n",
    "                 re.sub(r'[\\s+\\t]',' ',t) ]  for t in visible_texts if len(t.strip())>2]\n",
    "     \n",
    "    def SaveToCsv_FromUrl(self,url): \n",
    "        \"\"\"\n",
    "            Save text to a csv file. It calls Read_Oneurl()\n",
    "            Paramter: \n",
    "                url\n",
    "        \"\"\"\n",
    "        content=self.Read_Oneurl(url.strip()) \n",
    "        self.SaveToCsv(url, content) \n",
    "     \n",
    "    def SaveToCsv_FromResponse(self, url, response): \n",
    "        \"\"\"\n",
    "            Save to csv file. \n",
    "            Paramters: \n",
    "                url\n",
    "                response\n",
    "        \"\"\"\n",
    "        content= self.Get_Pagetext(response)\n",
    "        self.SaveToCsv(url, content) \n",
    "         \n",
    "    def SaveToCsv(self, url, content):\n",
    "        \"\"\"\n",
    "            Utility file. Save file to folder saveToFolder\n",
    "            File name convention: weburl+timestr. \n",
    "            Parameters: \n",
    "                url\n",
    "                content\n",
    "        \"\"\"\n",
    "        filename=url.replace('.', '_dot_').replace('/', '_').replace(':', '_')+'_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")+'.csv'\n",
    "        fullname=path.join(self.saveToFolder, filename)\n",
    "        try:\n",
    "            with open(fullname, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(['url', 'parent', 'ps', 'ns', 'text'])\n",
    "                for lll in content: \n",
    "                    lll.insert(0, url)\n",
    "                    writer.writerow(lll)  \n",
    "            self.files.append(fullname) \n",
    "        except: # IOError as e:\n",
    "            #print (\"I/O Error({0}): {1} in saving summaries\").format(e.errno, e.strerror) \n",
    "            pass\n",
    "           \n",
    "    def MoveFiles(self, destination_folder, source_folder=None,  pattern=None): \n",
    "        \"\"\"\n",
    "            Move File utility function. \n",
    "            Paramters: \n",
    "                destination_folder\n",
    "                source_folder\n",
    "                pattern\n",
    "        \"\"\"\n",
    "        if source_folder==None:\n",
    "            source_folder=self.saveToFolder\n",
    "        if source_folder==None:\n",
    "            print('Please specify folder with the files. ')\n",
    "            return\n",
    "\n",
    "        if destination_folder==None or path.isdir(destination_folder)==False:\n",
    "            print('Need a valid destination folder')\n",
    "            return\n",
    "        if pattern==None or pattern==\"\":\n",
    "            pattern=\"csv$\" \n",
    "        files = [f for f in os.listdir(self.saveToFolder) if re.match(pattern, f) and path.isfile(path.join(source_folder, f))]\n",
    "        for f in files:\n",
    "            shutil.move(path.join(source_folder, f), destination_folder)  \n",
    "    \n",
    "    def Merge(self, isRemoveRawFile=False ):         \n",
    "        \"\"\"\n",
    "            Merger files into one consolidated file. \n",
    "            drop duplicates. \n",
    "            Generate one file per college. \n",
    "            File name: supplied from constructor\n",
    "            \n",
    "            drop duplicates \n",
    "            use relative folder path only. ./file name\n",
    "        \"\"\"            \n",
    "        fullFileName=path.join(self.saveToFolder,self.mergedFile) \n",
    "             \n",
    "        files=[(path.join(self.saveToFolder, f), 1)  for f in os.listdir(self.saveToFolder) if re.match(r'^http.+\\d+.+csv$', f)]\n",
    "         \n",
    "        if len(files)>0:\n",
    "            if path.exists(fullFileName):\n",
    "                files.append((fullFileName, 0))\n",
    "            \n",
    "            if len(files)==1:\n",
    "                pd.read_csv(files[0][0] , sep='\\t').to_csv( fullFileName, index=False, sep='\\t', encoding='utf-8')\n",
    "            else:            \n",
    "                df_combined = pd.concat([pd.read_csv(f[0] , sep='\\t') for f in files]) \n",
    "\n",
    "                df_combined.drop_duplicates(inplace=True)\n",
    "                df_combined.to_csv( fullFileName, index=False, sep='\\t', encoding='utf-8')\n",
    "\n",
    "            if isRemoveRawFile:\n",
    "                for f in files:\n",
    "                    if f[1]==1:\n",
    "                        os.remove(f[0])\n",
    "        print('{} file(s) merged into {}\\n'.format(len(files)-1, fullFileName))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T06:44:45.580514Z",
     "start_time": "2020-01-09T06:44:45.574479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# About CollegeCrawl\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mandatory Input: \n",
      "            _collegeName: e.g., University of Toronto. \n",
      "                Targeted use case: input from batch file. \n",
      "            _rootUrl: e.g., https://www.uw.edu. \n",
      "                Targeted use case: input from batch file. \n",
      "                Both scheme and netloc has to be complete\n",
      "            _mergedFile: merge all crawled file, one csv per url, into one consolidated csv file.\n",
      "                Targeted use case: input from batch file.\n",
      "            _existingUrlFile: a csv file that stores url list. \n",
      "                Two columns: url and status code. \n",
      "                If the file doesnot exist, will generate it.\n",
      "                Targeted use case: input from batch file. \n",
      "        Optional Input:\n",
      "            _gapInSecond: if second >0 then the number of seconds. if -1 then 30 seconds\n",
      "            _maxPages: if -1 then 999 pages. if >0 then pages else 15 pages\n",
      "            _capUrls:  if -1 then 10000 pages. if >0 then  _capUrls else 2000 pages. Not to grab too many\n",
      "            _priorityKeywords=['apply', 'admission', 'application', 'deadline']\n",
      "                These words will be used for priority ranking. \n",
      "            _saveToFolder=None. Where to save the crawled csv files. \n",
      "                If None, then will take current work directory.  \n",
      "            _respectRobottxt=True, Not utilized for now. \n",
      "            _headers={'User-Agent':'Mozilla/5.0'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\"\n",
    "# About CollegeCrawl\n",
    "\"\"\" ))   \n",
    "print(CollegeCrawl.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T06:44:47.625033Z",
     "start_time": "2020-01-09T06:44:47.620036Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample way of calling: \n",
    "if 1==11:\n",
    "    c=CollegeCrawl('A.T. Still University of Health Sciences', 'http://www.atsu.edu','A_100000URL.csv', 'A_100000.csv')\n",
    "    c.GetallUrls()\n",
    "    summary=c.Show_Summaries(show=True)\n",
    "    c.Merge(isRemoveRawFile=True)\n",
    "    zero=summary[summary['status_code']=='0']['url'].sum()\n",
    "    t00=summary[summary['status_code']=='200']['url'].sum()\n",
    "    full=summary['url'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T06:44:49.816287Z",
     "start_time": "2020-01-09T06:44:49.806262Z"
    }
   },
   "outputs": [],
   "source": [
    "c=CollegeCrawl('Albertus Magnus College', 'http://www.albertus.edu','A_100020URL.csv', 'A_100020.csv')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T06:46:34.746509Z",
     "start_time": "2020-01-09T06:46:33.534609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albertus Magnus College\n",
      "Total unvisited and successfully visited pages is 0. The max is approx. 2000\n",
      "(Albertus Magnus College) http://www.albertus.edu : No files writen\n"
     ]
    }
   ],
   "source": [
    "k=0 \n",
    "ls_sum=[]\n",
    "c.GetallUrls()\n",
    "summary=c.Show_Summaries(show=False)\n",
    "if len(c.files)>0:\n",
    "    c.Merge(isRemoveRawFile=True)\n",
    "else:\n",
    "    print('({}) {} : No files writen'.format(c.collegeName, c.rootUrl))\n",
    "    \n",
    "try:\n",
    "    dataFile=path.join(c.saveToFolder, filename)\n",
    "    fileSizeinMB=path.getsize(dataFile)/1024/1024\n",
    "except: \n",
    "    fileSizeinMB=0.0\n",
    "\n",
    "zero=summary[summary['status_code']=='0']['url'].sum()\n",
    "t00=summary[summary['status_code']=='200']['url'].sum()\n",
    "full=summary['url'].sum()\n",
    "ls_sum.append([3, full,  t00, full-zero-t00, \"{0:.2f}\".format(fileSizeinMB)])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
