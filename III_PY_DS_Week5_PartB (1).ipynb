{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 bold=true>Jan 5, 2020 -- Part B\n",
    "<br><br>Excercise</font>\n",
    "\n",
    "<font size=5 color='Olive'>For the first 5 lessons, DS will be combined with Python due to insufficiencies of Concepts</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T20:26:25.582374Z",
     "start_time": "2020-01-05T20:26:25.577373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# 1. Flow Design: input, output\n",
       "# 2. Get the US college list and their websites\n",
       "# 3. Data preparation and sanity check\n",
       "# 4. Measurement: make sure data are as complete as possible\n",
       "# 5. Split tasks \n",
       "# 6. Merge and Pandas (from middle this week)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\"\n",
    "# 1. Flow Design: input, output\n",
    "# 2. Get the US college list and their websites\n",
    "# 3. Data preparation and sanity check\n",
    "# 4. Measurement: make sure data are as complete as possible\n",
    "# 5. Split tasks \n",
    "# 6. Merge and Pandas (from middle this week)\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T03:42:07.158501Z",
     "start_time": "2020-01-08T03:42:07.153468Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, Latex \n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import requests\n",
    "import ntpath\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "from pprint import pprint \n",
    "import shutil\n",
    "#!pip install tldextract\n",
    "import tldextract\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('C:/Coding/Get'))\n",
    "import CollegeContent as C\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T03:42:07.687761Z",
     "start_time": "2020-01-08T03:42:07.681797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# 1. Flow Design: input, output\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\"\n",
    "# 1. Flow Design: input, output\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T04:10:30.970943Z",
     "start_time": "2020-01-07T04:10:30.966952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "# 2. Get the US college list and their websites \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\" \n",
    "# 2. Get the US college list and their websites \n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T04:10:31.505915Z",
     "start_time": "2020-01-07T04:10:31.502953Z"
    }
   },
   "outputs": [],
   "source": [
    "url='https://www.university-list.net/us/universities-2000.htm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T03:40:31.523524Z",
     "start_time": "2020-01-08T03:40:31.513501Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    one page parsing only. \n",
    "    columns: \n",
    "    'collegeId','url', 'name', 'filename', 'urls', 'visitedurls', 'filesizeinMB'\n",
    "'''\n",
    "def AssignColleges(url='https://www.university-list.net/us/universities-2000.htm', geeks=['Allen', 'Angus', 'Jason', 'Wilson', 'Zilin']):\n",
    "    df_urls=pd.DataFrame()    \n",
    "    response=requests.get(url, headers={'User-Agent':'Mozilla/5.0'} )\n",
    "    soup=BeautifulSoup(response.text,  'lxml') #'html.parser')  \n",
    "    universities=[[re.sub(r'[\\/_+!@#$?\\\\\\s]+$', '', link['href']), link.text]  for link in   soup.find_all('a') if link.has_attr('href') \\\n",
    "     and re.match(r'^http.+edu.+', link['href'])]  \n",
    "    \n",
    "    randomints=np.random.randint(5, size=len(universities))\n",
    "    df_urls=pd.DataFrame([[w[0], w[1],  '', 0, 0, 0,0, num] for w, num in zip(universities, randomints)] , columns=['url', 'name', 'filename', 'urls', 'visitedurls', 'rejectedurls', 'filesizeinMB', 'randint'])  \n",
    "    filenameroot='./training/ContentAqusition_'\n",
    "    df_urls.reset_index(inplace=True)\n",
    "    df_urls.rename(columns={'index':'collegeId'}, inplace=True)\n",
    "    df_urls['filename']='A_'+(100000+df_urls['collegeId']).map(str)+'.csv' \n",
    "    df_urls['urlfile']='A_'+(100000+df_urls['collegeId']).map(str)+'url.csv' \n",
    "    \n",
    "    for i in range(len(geeks)):\n",
    "        filename='{}{}.csv'.format(filenameroot, geeks[i]) \n",
    "        df_urls[df_urls['randint']==i][['collegeId','url', 'name', 'filename','urlfile', 'urls', 'visitedurls','rejectedurls', 'filesizeinMB']].to_csv(filename,index = False)    \n",
    "    \n",
    "    #master file\n",
    "    filename='{}All.csv'.format(filenameroot) \n",
    "    df_urls[['collegeId','url', 'name', 'filename','urlfile', 'urls', 'visitedurls','rejectedurls', 'filesizeinMB']].to_csv(filename,index = False)    \n",
    "    gp= df_urls.groupby('randint')['collegeId'].count()\n",
    "    return gp.reset_index(), df_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T03:42:31.073076Z",
     "start_time": "2020-01-08T03:42:30.617971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collegeId</th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>filename</th>\n",
       "      <th>urls</th>\n",
       "      <th>visitedurls</th>\n",
       "      <th>rejectedurls</th>\n",
       "      <th>filesizeinMB</th>\n",
       "      <th>randint</th>\n",
       "      <th>urlfile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://www.atsu.edu</td>\n",
       "      <td>A.T. Still University of Health Sciences</td>\n",
       "      <td>A_100000.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A_100000url.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.acu.edu</td>\n",
       "      <td>Abilene Christian University</td>\n",
       "      <td>A_100001.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>A_100001url.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.abac.edu</td>\n",
       "      <td>Abraham Baldwin Agricultural College</td>\n",
       "      <td>A_100002.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A_100002url.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://www.academyart.edu</td>\n",
       "      <td>Academy of Art University</td>\n",
       "      <td>A_100003.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>A_100003url.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.adams.edu</td>\n",
       "      <td>Adams State College</td>\n",
       "      <td>A_100004.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>A_100004url.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   collegeId                        url  \\\n",
       "0          0        http://www.atsu.edu   \n",
       "1          1         http://www.acu.edu   \n",
       "2          2        http://www.abac.edu   \n",
       "3          3  http://www.academyart.edu   \n",
       "4          4       http://www.adams.edu   \n",
       "\n",
       "                                       name      filename  urls  visitedurls  \\\n",
       "0  A.T. Still University of Health Sciences  A_100000.csv     0            0   \n",
       "1              Abilene Christian University  A_100001.csv     0            0   \n",
       "2      Abraham Baldwin Agricultural College  A_100002.csv     0            0   \n",
       "3                 Academy of Art University  A_100003.csv     0            0   \n",
       "4                       Adams State College  A_100004.csv     0            0   \n",
       "\n",
       "   rejectedurls  filesizeinMB  randint          urlfile  \n",
       "0             0             0        0  A_100000url.csv  \n",
       "1             0             0        4  A_100001url.csv  \n",
       "2             0             0        1  A_100002url.csv  \n",
       "3             0             0        4  A_100003url.csv  \n",
       "4             0             0        3  A_100004url.csv  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum,df_urls=AssignColleges()\n",
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T03:43:59.624765Z",
     "start_time": "2020-01-08T03:43:59.618759Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mandatory Input: \n",
      "            _collegeName: e.g., University of Toronto. \n",
      "                Targeted use case: input from batch file. \n",
      "            _rootUrl: e.g., https://www.uw.edu. \n",
      "                Targeted use case: input from batch file. \n",
      "                Both scheme and netloc has to be complete\n",
      "            _mergedFile: merge all crawled file, one csv per url, into one consolidated csv file.\n",
      "                Targeted use case: input from batch file.\n",
      "            _existingUrlFile: a csv file that stores url list. e.g., A10000URL.csv. \n",
      "                Two columns: url and status code. \n",
      "                If the file doesnot exist, will generate it.\n",
      "                Targeted use case: input from batch file. \n",
      "        Optional Input:\n",
      "            _priorityKeywords=['apply', 'admission', 'application', 'deadline']\n",
      "                These words will be used for priority ranking. \n",
      "            _saveToFolder=None. Where to save the crawled csv files.  \n",
      "            _respectRobottxt=True, Not utilized for now. \n",
      "            _headers={'User-Agent':'Mozilla/5.0'}\n",
      "        Class Variables: \n",
      "            Gap_Insecond. default 1 second\n",
      "            Max_Pages. Number of pages per batch. \n"
     ]
    }
   ],
   "source": [
    "print(C.CollegeCrawl.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T03:43:24.156584Z",
     "start_time": "2020-01-08T03:43:05.218795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for college  A.T. Still University of Health Sciences\n",
      "\n",
      "Urls Identified from college A.T. Still University of Health Sciences.\n",
      "  status_code   url\n",
      "0           0  1998\n",
      "1         200    83\n",
      "2         404     4\n",
      "3         504     1\n",
      "4         999     5\n",
      "\n",
      "\n",
      "\n",
      "The following 5 file(s) are generated. \n",
      "['C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___jobs_dot_atsu_dot_edu_ahec_01_07_2020_19_43_09.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___live_dot_atsu_dot_edu_watch_01_07_2020_19_43_12.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___jobs_dot_atsu_dot_edu_asdoh_01_07_2020_19_43_20.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\https___www_dot_atsu_dot_edu_asdoh_01_07_2020_19_43_22.csv',\n",
      " 'C:\\\\Coding\\\\utility_git\\\\Notebooks\\\\http___www_dot_atsu_dot_edu_asdoh_admissions_contact_rep_dot_htm_01_07_2020_19_43_23.csv']\n",
      "\n",
      "5 file(s) merged into C:\\Coding\\utility_git\\Notebooks\\A_100000.csv.\n"
     ]
    }
   ],
   "source": [
    "k=0 \n",
    "for _, i in enumerate(df_urls.values):\n",
    "    collegeid,url, name, filename, urls, visitedurls, rejectedurls,filesizeinMB, randint, urlfile=i[0],i[1],i[2],i[3],i[4],i[5],i[6],i[7],i[8],i[9] \n",
    "    \n",
    "    c=C.CollegeCrawl(name, url,urlfile, filename)\n",
    "    c.GetallUrls()\n",
    "    summary=c.Show_Summaries(show=True)\n",
    "    c.Merge(isRemoveRawFile=True)\n",
    "    \n",
    "    zero=summary[summary['status_code']=='0']['url'].sum()\n",
    "    t00=summary[summary['status_code']=='200']['url'].sum()\n",
    "    full=summary['url'].sum()\n",
    "    break\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T03:41:28.151541Z",
     "start_time": "2020-01-07T03:41:28.148544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([0, 'http://www.atsu.edu',\n",
      "       'A.T. Still University of Health Sciences', 'A_100000', 0, 0, 0, 0],\n",
      "      dtype=object))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T23:04:48.418522Z",
     "start_time": "2020-01-05T23:04:48.415487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "# 3. Data preparation and sanity check \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\" \n",
    "# 3. Data preparation and sanity check \n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T22:02:26.336585Z",
     "start_time": "2020-01-05T22:02:26.331551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 5)\n",
      "(2, 4, 6)\n",
      "(3, 5, 7)\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\"\n",
    "## Build a loop to \n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T18:01:01.962803Z",
     "start_time": "2020-01-05T18:01:01.955839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "# 4. Measurement: make sure data are as complete as possible \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\" \n",
    "# 4. Measurement: make sure data are as complete as possible \n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T18:01:08.243722Z",
     "start_time": "2020-01-05T18:01:08.238681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "# 5. Split tasks \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\" \n",
    "# 5. Split tasks \n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T18:02:12.304935Z",
     "start_time": "2020-01-05T18:02:12.300014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "# 6. Merge and Pandas (from middle this week)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\n",
    "\"\"\" \n",
    "# 6. Merge and Pandas (from middle this week)\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
